{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in ./env/lib/python3.6/site-packages\n",
      "Requirement already satisfied: scikit-learn in ./env/lib/python3.6/site-packages (from sklearn)\n",
      "Requirement already satisfied: numpy>=1.11.0 in ./env/lib/python3.6/site-packages (from scikit-learn->sklearn)\n",
      "Requirement already satisfied: scipy>=0.17.0 in ./env/lib/python3.6/site-packages (from scikit-learn->sklearn)\n",
      "Requirement already satisfied: joblib>=0.11 in ./env/lib/python3.6/site-packages (from scikit-learn->sklearn)\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_forward(data, model, name):\n",
    "    model.eval()\n",
    "    batch_size = 1\n",
    "    start_time = time.time()\n",
    "    y_ans = []\n",
    "    y_pred = []\n",
    "    for batch in data.batch_iter(name,batch_size,False):\n",
    "        gaz_list, batch_word, batch_biword, batch_wordlen, batch_char, batch_charlen, batch_charrecover, batch_pos1, batch_pos2, ins_label, batch_label, mask, scope = batch\n",
    "        prob = model(gaz_list, batch_word, batch_biword, batch_wordlen, batch_char, batch_charlen, batch_charrecover, batch_pos1, batch_pos2, ins_label, scope)\n",
    "        \n",
    "        prob = prob.cpu().data.numpy()\n",
    "        assert batch_size == len(batch_label)\n",
    "        for bid in range(batch_size):\n",
    "            cur_ans = batch_label[bid]\n",
    "            cur_ans = list(set(cur_ans))\n",
    "            cur_prob = prob[bid]\n",
    "            y_ans.append(cur_ans)\n",
    "            y_pred.append(cur_prob)\n",
    "    \n",
    "    return y_ans,y_pred   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_file=data/FinRE/test.txt\n",
      "Data setting loaded from file:  models/FinRE.pkl.dset\n",
      "DATA SUMMARY START:\n",
      "     MAX SENTENCE LENGTH: 86\n",
      "     Number   normalized: True\n",
      "     Use          bigram: False\n",
      "     Word  alphabet size: 3069\n",
      "     Biword alphabet size: 94785\n",
      "     Char  alphabet size: 3068\n",
      "     Gaz   alphabet size: 26430\n",
      "     Label alphabet size: 44\n",
      "     Word embedding size: 100\n",
      "     Biword embedding size: 50\n",
      "     Char embedding size: 30\n",
      "     Gaz embedding size: 200\n",
      "     Norm     word   emb: True\n",
      "     Norm     biword emb: True\n",
      "     Norm     gaz    emb: False\n",
      "     Norm   gaz  dropout: 0.5\n",
      "     Train instance number: 0\n",
      "     Dev   instance number: 0\n",
      "     Test  instance number: 0\n",
      "     Hyperpara  iteration: 100\n",
      "     Hyperpara  batch size: 1\n",
      "     Hyperpara          lr: 0.015\n",
      "     Hyperpara    lr_decay: 0.05\n",
      "     Hyperpara     HP_clip: 5.0\n",
      "     Hyperpara  hidden_dim: 200\n",
      "     Hyperpara     dropout: 0.5\n",
      "     Hyperpara  lstm_layer: 1\n",
      "     Hyperpara      bilstm: False\n",
      "     Hyperpara         GPU: True\n",
      "     Hyperpara     use_gaz: True\n",
      "     Hyperpara fix gaz emb: False\n",
      "     Hyperpara    use_char: False\n",
      "DATA SUMMARY END.\n",
      "Merging data with same (head,tail,sent)...\n",
      "Finish merging\n",
      "Sort data...\n",
      "Finish sorting\n",
      "Total entities: 83235  Entities with multi-sense: 23556  Ratio: 28.300594701748064%\n",
      "Load model from  models/FinRE.pkl-233\n",
      "build batched bilstm-based encoder...\n",
      "embedding type <class 'torch.nn.modules.sparse.Embedding'>\n",
      "build LatticeLSTM...  forward , Fix emb: False  gaz drop: 0.5\n",
      "load pretrain word emb... (26430, 200)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "set_storage is not allowed on a Tensor created from .data or .detach().\nIf your intent is to change the metadata of a Tensor (such as sizes / strides / storage / storage_offset)\nwithout autograd tracking the change, remove the .data / .detach() call and wrap the change in a `with torch.no_grad():` block.\nFor example, change:\n    x.data.set_(y)\nto:\n    with torch.no_grad():\n        x.set_(y)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-029f3a24774d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Load model from '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMGLattice_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0my_ans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xuehp/git/Chinese_NRE/nn/framework.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMGLattice_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# MG-Lattice encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBiLstmEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Attentive classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xuehp/git/Chinese_NRE/nn/encoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'MGLattice'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;31m#print('Using MG-Lattice Encoder')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLatticeLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaz_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaz_alphabet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaz_emb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain_gaz_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHP_fix_gaz_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_lstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLatticeLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaz_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaz_alphabet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaz_emb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrain_gaz_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHP_fix_gaz_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'GRU'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xuehp/git/Chinese_NRE/nn/mglattice.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, hidden_dim, word_drop, word_alphabet_size, word_emb_dim, pretrain_word_emb, left2right, fix_word_emb, gpu, use_bias)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Initialize LSTM in different levels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiInputLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msense_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSenseLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordLSTMCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_emb_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xuehp/git/Chinese_NRE/nn/mglattice.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_size, hidden_size, use_bias)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'alpha_bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xuehp/git/Chinese_NRE/nn/mglattice.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mweight_hh_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mweight_hh_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_hh_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_hh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_hh_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0malpha_weight_hh_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: set_storage is not allowed on a Tensor created from .data or .detach().\nIf your intent is to change the metadata of a Tensor (such as sizes / strides / storage / storage_offset)\nwithout autograd tracking the change, remove the .data / .detach() call and wrap the change in a `with torch.no_grad():` block.\nFor example, change:\n    x.data.set_(y)\nto:\n    with torch.no_grad():\n        x.set_(y)"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# author: huihui\n",
    "# date: 2020/4/1 2:59 下午 \n",
    "\n",
    "import time\n",
    "import configure\n",
    "from utils.data_manager import *\n",
    "from nn.framework import MGLattice_model\n",
    "\n",
    "public_path = configure.public_path\n",
    "dataset = os.path.join(public_path, configure.dataset)\n",
    "test_file = os.path.join(dataset, configure.test_file)\n",
    "print('test_file={}'.format(test_file))\n",
    "\n",
    "data = load_data_setting(configure.savedset)\n",
    "data.generate_instance_with_gaz(test_file, 'test', load_mode='multilab-ins')\n",
    "\n",
    "model_dir = configure.loadmodel\n",
    "print('Load model from ', model_dir)\n",
    "model = MGLattice_model(data)\n",
    "model.load_state_dict(torch.load(model_dir, map_location='cpu'))\n",
    "y_ans, y_pred = evaluate_forward(data, model, 'test')\n",
    "\n",
    "print(y_ans)\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
